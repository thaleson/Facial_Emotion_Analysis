import cv2
import streamlit as st
import imutils
import numpy as np
from config import EMOTIONS, FACE_MODEL_ARCHITECTURE_PATH, FACE_MODEL_WEIGHTS_PATH, HAARCASCADE_PATH
from emotion_model import EmotionModel
from face_detector import FaceDetector
from tempfile import NamedTemporaryFile

def main():
    """
    Main function to run the Streamlit application for real-time facial emotion analysis.

    This function sets up the Streamlit page configuration, styles, and title. It allows the user to upload a video file,
    and upon pressing the "Start Processing" button, processes the video to detect and analyze facial emotions.
    The results are displayed in real-time, showing the detected emotions on the faces within the video.
    """
    st.set_page_config(page_title="üé¨ An√°lise de Emo√ß√µes Faciais", page_icon=":movie_camera:")

    st.markdown(
    f"""
    <style>
    {open("static/styles.css").read()}
    </style>
    """,
    unsafe_allow_html=True
)

    st.title("üé¨ An√°lise de Emo√ß√µes Faciais em Tempo Real e em V√≠deos")

    st.write("""
        üëã Ol√°! Bem-vindo ao nosso aplicativo de detec√ß√£o de emo√ß√µes faciais. 
        Aqui voc√™ pode carregar um v√≠deo e assistir em tempo real as emo√ß√µes identificadas nas faces detectadas. üòÉüò¢üò°
    """)

    video_file = st.file_uploader("üìπ Selecione um arquivo de v√≠deo", type=["mp4", "avi", "mov"])

    if video_file:
        st.write("üìÇ V√≠deo carregado com sucesso!")

        # Bot√£o para iniciar o processamento do v√≠deo
        if st.button("‚ñ∂Ô∏è Iniciar Processamento"):
            # Salvar o v√≠deo temporariamente
            temp_video = NamedTemporaryFile(delete=False)
            temp_video.write(video_file.read())
            temp_video.close()
            
            # Inicializar modelos
            emotion_model = EmotionModel(FACE_MODEL_ARCHITECTURE_PATH, FACE_MODEL_WEIGHTS_PATH)
            face_detector = FaceDetector(HAARCASCADE_PATH)

            video_capture = cv2.VideoCapture(temp_video.name)
            frame_out = st.empty()  # √Årea para exibir os frames

            # Processar v√≠deo frame a frame
            while True:
                ret, frame = video_capture.read()
                if not ret:
                    st.write("üìΩÔ∏è Fim do v√≠deo.")
                    break

                frame = imutils.resize(frame, width=800)
                gray, detected_faces = face_detector.detect_faces(frame)

                for face in detected_faces:
                    (x, y, w, h) = face
                    if w > 100:
                        extracted_face = face_detector.extract_face_features(gray, face, (0.075, 0.05))
                        predictions = emotion_model.predict(extracted_face)
                        prediction_result = np.argmax(predictions)
                        expression_text = EMOTIONS[prediction_result]
                        
                        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)
                        cv2.putText(frame, expression_text, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (250, 250, 250), 2)
                
                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                frame_out.image(frame, channels="RGB", use_column_width=True)
                
                # Adicionar um pequeno atraso para simular a reprodu√ß√£o em tempo real
                # O valor do atraso pode ser ajustado conforme necess√°rio
                st.time.sleep(0.03)  # Aproximadamente 30 FPS

            video_capture.release()
            st.write("‚úÖ Processamento conclu√≠do!")

if __name__ == '__main__':
    main()

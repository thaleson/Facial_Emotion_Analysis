# main.py
import cv2
import streamlit as st
import imutils
import numpy as np
from config import EMOTIONS, FACE_MODEL_ARCHITECTURE_PATH, FACE_MODEL_WEIGHTS_PATH, HAARCASCADE_PATH
from emotion_model import EmotionModel
from face_detector import FaceDetector
from tempfile import NamedTemporaryFile





def main():
    st.set_page_config(page_title="ðŸŽ¬ AnÃ¡lise de EmoÃ§Ãµes Faciais", page_icon=":movie_camera:")

    st.markdown(
    f"""
    <style>
    {open("static/styles.css").read()}
    </style>
    """,
    unsafe_allow_html=True
)

    
    st.title("ðŸŽ¬ AnÃ¡lise de EmoÃ§Ãµes Faciais em Tempo Real e em VÃ­deos")

    st.write("""
        ðŸ‘‹ OlÃ¡! Bem-vindo ao nosso aplicativo de detecÃ§Ã£o de emoÃ§Ãµes faciais. 
        Aqui vocÃª pode carregar um vÃ­deo e assistir em tempo real as emoÃ§Ãµes identificadas nas faces detectadas. ðŸ˜ƒðŸ˜¢ðŸ˜¡
    """)

    video_file = st.file_uploader("ðŸ“¹ Selecione um arquivo de vÃ­deo", type=["mp4", "avi", "mov"])

    if video_file:
        st.write("ðŸ“‚ VÃ­deo carregado com sucesso!")

        # BotÃ£o para iniciar o processamento do vÃ­deo
        if st.button("â–¶ï¸ Iniciar Processamento"):
            # Salvar o vÃ­deo temporariamente
            temp_video = NamedTemporaryFile(delete=False)
            temp_video.write(video_file.read())
            temp_video.close()
            
            # Inicializar modelos
            emotion_model = EmotionModel(FACE_MODEL_ARCHITECTURE_PATH, FACE_MODEL_WEIGHTS_PATH)
            face_detector = FaceDetector(HAARCASCADE_PATH)

            video_capture = cv2.VideoCapture(temp_video.name)
            frame_out = st.empty()

            while True:
                ret, frame = video_capture.read()
                if not ret:
                    st.write("ðŸ“½ï¸ Fim do vÃ­deo.")
                    break

                frame = imutils.resize(frame, width=800)
                gray, detected_faces = face_detector.detect_faces(frame)

                for face in detected_faces:
                    (x, y, w, h) = face
                    if w > 100:
                        extracted_face = face_detector.extract_face_features(gray, face, (0.075, 0.05))
                        predictions = emotion_model.predict(extracted_face)
                        prediction_result = np.argmax(predictions)
                        expression_text = EMOTIONS[prediction_result]
                        
                        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)
                        cv2.putText(frame, expression_text, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (250, 250, 250), 2)
                
                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                frame_out.image(frame, channels="RGB", use_column_width=True)

            video_capture.release()
            st.write("âœ… Processamento concluÃ­do!")

if __name__ == '__main__':
    main()
